Time Complexity: a measure how fast a solution/algorithm runs.

Space Complexity: a measure of how much auxiliary memory an algorithm takes up. It's expressed using Big O notation.

Complexity Analysis: involves finding both the time and space complexity of an algorithm. 
                     It is effectively used to determine how "good" an algorithm is and whether it's "better" than another one.


-- MEMORY --

Memory is a bounded canvas of memory slots that can store data. It is possible to run out of memory due to being bounded.

Your program will always store a variable in a memory or series of memory slots that are free (not taken up)
If a variable needed more than 1 memory slot to store information it would store it in back to back memory slots

Memory is made up of BITS which are 0s and 1s. We store data in the form of bits.
Memory has memory addresses which are made up of binary numbers.

1 memory slot can hold 8 bits which is called a byte. 8 bits = 1 byte 
1 byte is 2^8 since their are 8 bits and each bit has an option of 0 or 1 meaing  1 byte can only have 256 potential data values
How would we store more data values?
Just increase the number of bits that represent that data value
An int is represented using 32 bits (4 bytes) vs a long int which is represented using 64 bits (8 bytes)
A 32 bit int would take up 4 memory slots (1byte/slot) and a 64 bit int would take up 8 slots (1byte/slot)
When storing a list of ints these int will also have to be in contiguous memory slots. For example, [1,2,3] would take up
12 contiguous memory slots

Fix-Width Integer: an int represented by a fixed amount of bits (32 vs 64). Once decided what type of int you are going to be using
                   it will always take up the amount of memory regardless of how large the number is. A 32 bit int of 1 will take up 4 memory slots
                   and a 32 bit int of 1000 will also take up 4 memory slots

Pointers: memory addresses can point to other memory addresses that store a relevant piece of data 


https://en.wikipedia.org/wiki/Binary_number

-- BIG O NOTATION --

We care about how fast an algorithm runs as the size of the input increases.

Asymptotic Analysis: refers to how our function performs as we get closer to inifinty or some very large number

Big O Complexities refer to the worst case complexity of an algorithm

Time Complexities:

O(1) or constant time: as the size of the input increases the speed of the algorithm is constant.
O(log(N)) or logarithmic
O(N) or linear time: as the size of the input increases the speed of the algorithm increases linearly.
O(Nlog(n)) or log-linear
O(N^2) or quadratic 
O(N^3) or cubic 
O(2^N) or exponential 
O(N!) or factorial

-- LOGARITHM --

Logarithm: used to describe the complexity analysis of algorithms.

Its usage always implies a logarithm of base 2.

log(n) = y IF AND ONLY IF 2^y = n

Basically whenever our inputs doubles in size then an algorithm of O(log(n)) where n is the size of the input.
Then whenever the input doubles in size the number of operations needed to complete the algo only increases by 1.

In the other hand an algo with a linear time complexity would see its number of operations double if its input size doubled.

Am I eliminating half of the input at every step of my function?
If I double the size of my input, am I only going to be performing one extra operation?

If you answered yes to both questions above then we are most likely dealing with an algorithm that runs in log(n) time

-- ARRAYS --

Static Array vs Dynamic Array

Static Array: an array that allocates a fixed amount of memory to be used for storing the array's values.
              When you append a value to a static array, the array gets copied and allocated new memory for it,
              accoutning for the extra space needed for the newly appended value. This is done in linear time.

Dynamic Array: an array that preemptively allocates double the amount of memory needed to store the array's values.
               Unlike a static array, appending a value to a dynamic array is done in constant time until the allocated memory
               is filled up. Once the array fills up, the array is then copied and doubles the memory once again allocated for it.
               This implementation leads to an amortized constant time insertion at end operation.

Array's standard operations and their time complexities:
 - Accessing a value at a given index: O(1)
 - Updating a value at a given index: O(1)
 - Inserting a value at the beginning: O(n)
 - Inserting a value in the middle: O(n)
 - Inserting a value at the end:
    - amortized O(1) when dealing with a dynamic array
    - O(n) when dealing with a static array
 - Removing a value at the beginning: O(n) this is because all the values in the array have to shift left
 - Removing a value in the middle: O(n) once again the values have to shift
 - Removing a value at the end: O(1) no shifting since value is at the end
 - Copying an array: O(n) array has to be traversed and copied

-- LINKED LISTS --

 Singly linked list: typically exposes its head to its user for easy access. To find a node
                     in you must traverse through all the of nodes leading up to the node in question.
                     Adding and removing nodes simply involves overwriting next pointers (assuming that 
                     you have access to the node right before the node that you're adding or removing)

Singly Linked List standard operations and their time complexities:
 - Accessing the head: O(1)
 - Accessing the tail: O(n)
 - Accessing a middle node: O(n) because we have to traverse through the linked list 
 - Inserting/removing the head: O(1) because we are only overriding one memory address whic is the head
 - Inserting/removing the tail: O(n) to access + O(1) to insert/remove
 - Inserting/removing a middle node: O(n) to access + O(1) to insert/remove
 - Searching for a value: O(n)

Creating new pointers is very cheap and fast at O(1)

Doubly Linked List: each node in a doubly linked list has a pointer to the previous node in the linked list.
                    The previous node is typically store in a prev property.

Just as the next property of a doubly link lists tail points to the null value, so too does the prev property of
a doubly linked list's head.

null <- 0 <-> 1 <-> 2 <-> 3 <-> 4 <-> 5 <-> null

Doubly linked lists standard operations and their time complexities:
 - Accessing the head: O(1)
 - Accessing the tail: O(1)
 - Accessing the middle node: O(n)
 - Inseting/removing the head: O(1)
 - Inserting/removing the tail: O(1)
 - Inserting/removing a middle node: O(n) to access + O(1) to insert/remove
 - Searching for a value: O(n)

Circular Linked List: has no clear head or tail, because its tail points to its head, effectively forming a closed circle.

A circular linked list can be either a singly circular linked list or a doubly circular linked list.

When wanting to get the index(position) of an element in an array and a value do not use range(len(arr))
Instead use enumerate to get index and value at the same time and more cleanly

-- TREE --
A data structure that consists of nodes, each with some value and pointers to child-nodes, which recursively form subtrees in the tree.

The first node in a tree is referred to as the root of the tree, while the nodes at the bottom of a tree are referred to as lead nodes
or simply leaves.

The paths between the root of a tree and its leaves are called branches, and the height of a tree is the length of its longest branch.

The depth of a tree node is its distance from its tree's root aks as the node's level in the tree

A tree is effectively a graph that's connected, directed and acyclic, that has nan explicit root node, and whose nodes all have a single
parent (except for the root node). Note that in most implementation of trees, tree nodes don't have a pointer to their parent, but they can if
desired.

There are many types of trees includes binary tree, heaps and tries

* Binary Tree *

A tree whose nodes have up two child-nodes

The structure of a binary tree is such that many of its operations have a logarithmic time complexity, making the binary tree an incredibly attractive
and commonly used data structure.

* K-ary Tree *

A tree whose nodes have up to k child-nodes. A binary tree is a k-ary tree where k == 2

* Perfect Binary Tree *

A binary tree whose interior nodes all have two child-nodes and whose lead nodes all have the same depth.

* Complete Binary Tree * 

A binary tree that's almost perfect; its interior nodes all have two child nodes, but its leaf nodes don't necessarily
all have the same dept. Furthermore, the nodes in the last level of a complete biary tree are as far left as possible

* Balanced Binary Tree *

A binary tree whose nodes all have left and right subtrees whose heights differ by no more than 1.

A balanced binary tree is such that the logarithmic time complexity of its operations is maintained.

* Full Binary Tree *

A binary tree whose nodes all have either two child nodes or zero child nodes.


-- HASH TABLES --

A data structure that provides fast insertion, deletion and lookup of key/value pairs

Under the hood, a hash table uses a dynamic array of linked lists to efficiently store key/ value pairs.
When inserting a key/value pair, a hash function first maps the key, which is typically a string (or any data
that can be hashed, depending on the implementation of the hash table), to an integer value and by extension 
to an index in the underlying dynamic array. Then the value associated with the key is added to the linked list 
stored at that index in the dynamic array, and a reference to the key is also stored with the value.

Hash tables rely on hgihly optimized hash functions to minimize the number of collisions that occur when storing values:
cases where two keys map to the same index.

Hash table's standard operations and their corresponding time complexities:
 - Insterting a key/value pair: O(1) on average; O(n) in the worse case
 - Removing a key/value pair: O(1) on average; O(n) in the worst case 
 - Looking up a key: O(1) on average; O(n) in the worse case

The worst-case linear time operations occur when a hash table experiences a lot of collision, leading to long linked lists
internally, which take O(n) time to traverse.

Typically in interviews we assume that the hash functions employed by hash tables are so optimizes that collisions are extremely
rare and constant time operations are all but guranteed.

-- STACKS AND QUEUES --

STACK: an array-like data structure whose elements follow the LIFO rule: Last In, First out

A stack can be thought of as a stack of books on a table: the last book that's placed on the stack of books is the first one that's taken
off the stack.

Stack's standard operations and their corresponding time complexities:
 - Pushing an element onto the stack: O(1)
 - Popping an element off the stack: O(1)
 - Peeking at the element on the top of the stack: O(1)
 - Searching for an element in the stack: O(n)

A stack is typically implemented with a dynamic array or with a singly linked list.

QUEUE: an array-like data structure whose elements follow the FIFO rule: First In, First Out

A queue is often compared to a group of people standing in line to purchase items at a store: the first person to get in line is the first one
to purchase items and to get out of the queue 

Queue's standard operations and their corresponding time complexities:
 - Enqueing an element into the queue: O(1)
 - Dequeuing an element out of the queue: O(1)
 - Peeking at the element at the fron of the queue: O(1)
 - Searching for an element in the queue: O(n)

A queue is typically implemented with a doubly linked list

-- STRING --

Strings are stored in memory as arrays of integers, where each character in a given string is mapped to an integer via some character-encoding 
standard like ASCII.

In most programming languages (with the exception of C++), strings are immutable, meaning that they can't be edited after creation. This also
means that simple operations like appending a character to a string are more expensive than they might appear.

String's standard operations and theur corresponding time complexities:
 - Traverse through a string: O(n) Time | O(1) space
 - Copy a string: O(n) space and time
 - Get a string or char: O(1) space and time 

-- GRAPHS --

Graph: a collection of nodes or values called vertices that might be related; relation between vertice are called edges.

Graph Cycle: a cycle occurs in a grpah when three or more vertices in the graph are connected so as to form a closed loop.
             Note: the definition of a graph cycle is sometimes broadened to include cycles of length two or one; in the context
             of coding interviews, when dealing with questions that involve graph cycles, it's important to clarify what exactly
             constitues a cycle.

Acyclic Graph: a graph that has no cycles.

Cyclic Graph: a graph that has at least one cycle.

Directed Graph: a graph whose edges are directed, meaning that they can only be traversed in one direction, which is specified.
                
                For example, a graph of airports and flights would likely be directed, since a flight specifically goes from one airport to another
                (i.e., it has a direction), without necessarily implying the presence of a flight in the opposite direction.

Undirected Graph: a graph whose edges are undirected, meaning that they can be traversed in both directions.
                  
                  For example, a graph of friends would likely be undirected, since a frinedship is, by nature, bidirectional.

Connected Graph: a graph is connected if for every pair of vertices in the graph, there's a path of one or more esges connecting the given vertices.
                 
                 In the case of a directed graph the graph is:
                  - STRONGLY CONNECTED if there are bidirectional connections between the vertices of every pair of vertices (i.e. for every vertex-pair 
                  (u, v) you can reach v from u and u from v)

                  - WEAKLY CONNECTED if there are connections (but not necessarily bidirectional ones) between the vertices of every pair of vertices.

A graph that isn't connected is said to be DISCONNECTED.

-- HEAPS --
Heaps are typically visualized as trees, more specifically a complete binary tree. There are two different types of heaps:

Min Heap: the value of node i is greater than or equal to its parent (min value will be at the root node)
Max Heap: the value of node i is less than or equal to its parent (max value will be at the root node)

Uses:
 - heapsort --> max heap
 - priority queues --> min heap 

Height: O(log n)
Root of the tree is at index 1 of the array
Left child: left(i) = 2*i 
Right child: right(i) = 2*i + 1
Parent of node: parent(i) = floor(i//2)

Operations and their time complexity:
 - Insert: O(log n)
 - Pop: O(log n)
 - Min/Max: O(1)

Max_heapify is used to have the largest value above the respective smallest value which uses recursion to
complete this function. In this scenario we try to have the smallest value put in the correct position which
is called "float down". Running time of max heapify is O(log n) due to the height of the complete binary tree

Build Max Heap - wrapper function that calls max-heapify 
   We can determie the leaves of the tree using this formula: leaves = A[floor(n//2) + 1] to A[n]
   Uses bottom up approach since we are not starting at the root node
   Run time: O(n)

-- BFS -- 
To find the shortest path using BFS ask yourself these 2 questions:
   1. Is there a path from node A to node B?
   2. What is the shortest path from node A to node B?

BFS not only finds a path from A to B, it also finds the shortest path.

Use Queues data structure to implement BFS algo
   - queues have two operations: enque (add an item to the queue) and deque (take an item off the queue)
   - queues use the FIFO data structure: First In, First Out

Running time of BFS:
   - O(number of edges): if you search your entire network for a mango seller, that means you'll follow each edge
   - You also keep a queue of every person to search. Adding one person to the queue take constant time O(1). Doing
      this for every person will take O(number of nodes). BFS takes O(number of nodes + number of edges), and its more
      commonly written as O(V+E) (V for number of vertices, E for number of edges)

-- Binary Search Trees --
In a Binary Search Tree the values to the left of the node are smaller in value and nodes to the right
are larger in value. 