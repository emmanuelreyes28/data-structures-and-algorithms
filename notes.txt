Time Complexity: a measure how fast a solution/algorithm runs.

Space Complexity: a measure of how much auxiliary memory an algorithm takes up. It's expressed using Big O notation.

Complexity Analysis: involves finding both the time and space complexity of an algorithm. 
                     It is effectively used to determine how "good" an algorithm is and whether it's "better" than another one.


-- MEMORY --

Memory is a bounded canvas of memory slots that can store data. It is possible to run out of memory due to being bounded.

Your program will always store a variable in a memory or series of memory slots that are free (not taken up)
If a variable needed more than 1 memory slot to store information it would store it in back to back memory slots

Memory is made up of BITS which are 0s and 1s. We store data in the form of bits.
Memory has memory addresses which are made up of binary numbers.

1 memory slot can hold 8 bits which is called a byte. 8 bits = 1 byte 
1 byte is 2^8 since their are 8 bits and each bit has an option of 0 or 1 meaing  1 byte can only have 256 potential data values
How would we store more data values?
Just increase the number of bits that represent that data value
An int is represented using 32 bits (4 bytes) vs a long int which is represented using 64 bits (8 bytes)
A 32 bit int would take up 4 memory slots (1byte/slot) and a 64 bit int would take up 8 slots (1byte/slot)
When storing a list of ints these int will also have to be in contiguous memory slots. For example, [1,2,3] would take up
12 contiguous memory slots

Fix-Width Integer: an int represented by a fixed amount of bits (32 vs 64). Once decided what type of int you are going to be using
                   it will always take up the amount of memory regardless of how large the number is. A 32 bit int of 1 will take up 4 memory slots
                   and a 32 bit int of 1000 will also take up 4 memory slots

Pointers: memory addresses can point to other memory addresses that store a relevant piece of data 


https://en.wikipedia.org/wiki/Binary_number

-- BIG O NOTATION --

We care about how fast an algorithm runs as the size of the input increases.

Asymptotic Analysis: refers to how our function performs as we get closer to inifinty or some very large number

Big O Complexities refer to the worst case complexity of an algorithm

Time Complexities:

O(1) or constant time: as the size of the input increases the speed of the algorithm is constant.
O(log(N)) or logarithmic
O(N) or linear time: as the size of the input increases the speed of the algorithm increases linearly.
O(Nlog(n)) or log-linear
O(N^2) or quadratic 
O(N^3) or cubic 
O(2^N) or exponential 
O(N!) or factorial

-- LOGARITHM --

Logarithm: used to describe the complexity analysis of algorithms.

Its usage always implies a logarithm of base 2.

log(n) = y IF AND ONLY IF 2^y = n

Basically whenever our inputs doubles in size then an algorithm of O(log(n)) where n is the size of the input.
Then whenever the input doubles in size the number of operations needed to complete the algo only increases by 1.

In the other hand an algo with a linear time complexity would see its number of operations double if its input size doubled.

Am I eliminating half of the input at every step of my function?
If I double the size of my input, am I only going to be performing one extra operation?

If you answered yes to both questions above then we are most likely dealing with an algorithm that runs in log(n) time

-- ARRAYS --

Static Array vs Dynamic Array

Static Array: an array that allocates a fixed amount of memory to be used for storing the array's values.
              When you append a value to a static array, the array gets copied and allocated new memory for it,
              accoutning for the extra space needed for the newly appended value. This is done in linear time.

Dynamic Array: an array that preemptively allocates double the amount of memory needed to store the array's values.
               Unlike a static array, appending a value to a dynamic array is done in constant time until the allocated memory
               is filled up. Once the array fills up, the array is then copied and doubles the memory once again allocated for it.
               This implementation leads to an amortized constant time insertion at end operation.

Array's standard operations and their time complexities:
 - Accessing a value at a given index: O(1)
 - Updating a value at a given index: O(1)
 - Inserting a value at the beginning: O(n)
 - Inserting a value in the middle: O(n)
 - Inserting a value at the end:
    - amortized O(1) when dealing with a dynamic array
    - O(n) when dealing with a static array
 - Removing a value at the beginning: O(n) this is because all the values in the array have to shift left
 - Removing a value in the middle: O(n) once again the values have to shift
 - Removing a value at the end: O(1) no shifting since value is at the end
 - Copying an array: O(n) array has to be traversed and copied

 -- LINKED LISTS --

 Singly linked list: typically exposes its head to its user for easy access. To find a node
                     in you must traverse through all the of nodes leading up to the node in question.
                     Adding and removing nodes simply involves overwriting next pointers (assuming that 
                     you have access to the node right before the node that you're adding or removing)

Singly Linked List standard operations and their time complexities:
 - Accessing the head: O(1)
 - Accessing the tail: O(n)
 - Accessing a middle node: O(n)
 - Inserting/removing the head: O(1)
 - Inserting/removing the tail: O(n) to access + O(1)
 - Inserting/removing a middle node: O(n) to access + O(1)
 - Searching for a value: O(n)